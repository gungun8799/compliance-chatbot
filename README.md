# RAG FAQ Chatbot (Production-Ready) ğŸš€

This repository contains a Retrieval-Augmented Generation (RAG) FAQ chatbot using Qdrant for vector search and Chainlit for the frontend.

## ğŸŒŸ Features
- ğŸ—‚ **Chat Profile**: Personalized user interactions.
- ğŸ§  **Chat Memory**: Stores past interactions for continuity.
- ğŸ’¾ **Data Persistence**: Ensures long-term storage of chatbot data.
- ğŸ‘ **Human Feedback**: Allows users to provide feedback on chatbot responses.
- ğŸ¤” **Thinking Step (Same as Deepseek-R1)**: Implements structured reasoning for responses.
- ğŸ” **Authentication**: Secure user authentication for access control.

## ğŸ”§ Technologies Used

- **Groq**: Utilized for running Large Language Models (LLMs) with high efficiency and speed.
- **Cohere**: Employed for generating text embedding services.
- **LlamaIndex**: Serves as the AI workflow backend.
- **Chainlit**: Provides the frontend framework for the chatbot.

## ğŸ“¸ Thinking Process Screenshot

Below is a screenshot demonstrating the chatbot's reasoning process:

![Thinking Process](screenshots/thinking-page.png)

## ğŸš€ Deployment Instructions

### 1. Create AI Network
Before deploying, ensure you have the required Docker network:
```sh
docker network create --driver bridge ai
```

### 2. Deploy Services
To deploy the entire stack, run:
```sh
docker compose -p faq_chatbot up --build -d
```

To build and start specific services:
```sh
docker compose -p faq_chatbot up --build -d postgres prisma localstack ai-chatbot
```

## ğŸ“‚ Project Structure
```
rag-faq-chatbot/
â”‚â”€â”€ chainlit_app/   # Chainlit-based chatbot
â”‚â”€â”€ documents/      # FAQ JSON documents
â”‚â”€â”€ localstack/     # Localstack initialization
â”‚â”€â”€ prisma/         # Prisma ORM setup
â”‚â”€â”€ docker-compose.yml  # Docker configuration
â”‚â”€â”€ embedding.ipynb  # Script for embedding documents into Qdrant
â”‚â”€â”€ .env.example     # Example environment variables
```

## ğŸ”— Service Overview

| Service       | Port Mappings | Description |
|--------------|--------------|-------------|
| **Phoenix**  | 3000, 4317   | LLM tracing and monitoring |
| **Redis**    | 6379, 8001   | Redis database for storing chat history |
| **Qdrant**   | 6333, 6334   | Vector database for embeddings |
| **Postgres** | 5432         | SQL database for structured data |
| **Prisma**   | 5555         | Prisma ORM for database interactions |
| **Localstack** | 4566       | Mock AWS services |
| **Chainlit App** | 8000     | Chatbot UI & API |

## ğŸ“ Post-Deployment Setup
After deploying the services, **run `embedding.ipynb`** to create the vector database in Qdrant from FAQ documents.

This script will:
- Parse JSON FAQs into documents
- Generate embeddings using Cohere API
- Store embeddings into Qdrant

After that, you can access the Chainlit app at:
**[http://localhost:8000/login](http://localhost:8000/login)**

## âš™ï¸ Environment Variables
Copy `.env.example` to `.env` and update the values before running the chatbot:
```sh
cp .env.example .env
```

## ğŸ”§ Development & Debugging
To bring down the stack:
```sh
docker compose -p faq_chatbot down
```
To check logs for a specific service:
```sh
docker logs -f <container_name>
```