{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def section_aware_split(text: str, max_chunk_len: int = 1500) -> list:\n",
    "    \"\"\"\n",
    "    Chunk a Markdown-style document into hierarchical sections (using #, ##, ###) \n",
    "    and return structured chunks with section path and level.\n",
    "    \"\"\"\n",
    "    import re\n",
    "\n",
    "    lines = text.splitlines()\n",
    "    chunks = []\n",
    "    current_chunk_lines = []\n",
    "    current_path = []\n",
    "\n",
    "    def flush_chunk():\n",
    "        if not current_chunk_lines:\n",
    "            return\n",
    "        content = \"\\n\".join(current_chunk_lines).strip()\n",
    "        if content:\n",
    "            chunks.append({\n",
    "                \"section_path\": current_path.copy(),\n",
    "                \"level\": len(current_path),\n",
    "                \"content\": content\n",
    "            })\n",
    "\n",
    "    for line in lines:\n",
    "        header_match = re.match(r\"^(#{1,6})\\s+(.*)\", line)\n",
    "        if header_match:\n",
    "            # New header found\n",
    "            flush_chunk()\n",
    "            level = len(header_match.group(1))\n",
    "            title = header_match.group(2).strip()\n",
    "            current_path = current_path[:level - 1] + [title]\n",
    "            current_chunk_lines = [line]\n",
    "        else:\n",
    "            current_chunk_lines.append(line)\n",
    "\n",
    "    flush_chunk()\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import nest_asyncio\n",
    "\n",
    "from llama_index.core import Document, VectorStoreIndex, StorageContext\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core.vector_stores import VectorStoreQueryResult\n",
    "from qdrant_client import QdrantClient, AsyncQdrantClient\n",
    "from llama_index.embeddings.cohere import CohereEmbedding\n",
    "from llama_index.core import Settings\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import os\n",
    "nest_asyncio.apply()\n",
    "load_dotenv(dotenv_path=\".env.dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "\n",
    "def extract_tables_as_markdown(docx_path):\n",
    "    doc = Document(docx_path)\n",
    "    markdown_tables = []\n",
    "    for table in doc.tables:\n",
    "        rows = []\n",
    "        for row in table.rows:\n",
    "            cells = [cell.text.strip() for cell in row.cells]\n",
    "            rows.append(\"| \" + \" | \".join(cells) + \" |\")\n",
    "        if rows:\n",
    "            header = rows[0]\n",
    "            separator = \"| \" + \" | \".join([\"---\"] * len(table.columns)) + \" |\"\n",
    "            markdown_table = \"\\n\".join([header, separator] + rows[1:])\n",
    "            markdown_tables.append(markdown_table)\n",
    "    return markdown_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12 .docx file(s):\n",
      "  ‚Ä¢ documents/‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏™‡∏¥‡∏ô‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à B2B.docx\n",
      "  ‚Ä¢ documents/‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏° ‡∏Ñ‡∏±‡∏î‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤ ‡∏Å‡∏≤‡∏£‡∏ï‡πà‡∏≠‡∏™‡∏±‡∏ç‡∏ç‡∏≤ ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏´‡∏ô‡∏µ‡πâ.docx\n",
      "  ‚Ä¢ documents/‡∏≠‡∏≥‡∏ô‡∏≤‡∏à‡∏≠‡∏ô‡∏∏‡∏°‡∏±‡∏ï‡∏¥‡∏£‡∏≤‡∏¢‡∏à‡πà‡∏≤‡∏¢‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ.docx\n",
      "  ‚Ä¢ documents/‡∏Å‡∏≤‡∏£‡πÄ‡∏ö‡∏¥‡∏Å‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô.docx\n",
      "  ‚Ä¢ documents/Policy FAQ.docx\n",
      "  ‚Ä¢ documents/B2B Others.docx\n",
      "  ‚Ä¢ documents/‡∏≠‡∏≥‡∏ô‡∏≤‡∏à‡∏≠‡∏ô‡∏∏‡∏°‡∏±‡∏ï‡∏¥‡∏£‡∏≤‡∏¢‡∏à‡πà‡∏≤‡∏¢‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Purchase Requisition.docx\n",
      "  ‚Ä¢ documents/‡∏≠‡∏≥‡∏ô‡∏≤‡∏à‡∏≠‡∏ô‡∏∏‡∏°‡∏±‡∏ï‡∏¥ DoA ‡πÅ‡∏•‡∏∞ LoA.docx\n",
      "  ‚Ä¢ documents/‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏π‡πà‡∏Ñ‡πâ‡∏≤ ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏à‡πà‡∏≤‡∏¢‡πÄ‡∏á‡∏¥‡∏ô (Trade).docx\n",
      "  ‚Ä¢ documents/‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÅ‡∏•‡∏∞‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏π‡πà‡∏Ñ‡πâ‡∏≤ (Non-trade).docx\n",
      "  ‚Ä¢ documents/‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏≠‡∏∑‡πà‡∏ô‡πÜ‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏™‡∏¥‡∏ô‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à B2B.docx\n",
      "  ‚Ä¢ documents/‡πÄ‡∏á‡∏¥‡∏ô‡∏•‡∏á‡∏ó‡∏∏‡∏ô‡πÉ‡∏ô‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£.docx\n",
      "Loaded 12 raw Document(s) from all .docx files.\n",
      "After splitting, we have 545 chunked Documents (nodes).\n"
     ]
    }
   ],
   "source": [
    "from llama_index.readers.file import DocxReader\n",
    "from llama_index.core.schema import Document\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "\n",
    "# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "# CONFIGURATION\n",
    "# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "DOCX_FOLDER = \"documents/\"\n",
    "SHAREPOINT_BASE_URL = \"https://cpaxtra.sharepoint.com/sites/forms-library\"\n",
    "\n",
    "# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "# SECTION TITLE HELPER\n",
    "# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "def extract_section_title(chunk: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract section title from chunk marked as [SECTION] ... or fallback to first line.\n",
    "    \"\"\"\n",
    "    match = re.search(r\"\\[SECTION\\] (.*?)\\n\", chunk)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    # fallback to first non-empty line\n",
    "    lines = [line.strip() for line in chunk.splitlines() if line.strip()]\n",
    "    return lines[0] if lines else \"unknown\"\n",
    "\n",
    "# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "# STEP 1: Discover all .docx files\n",
    "# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "all_paths = glob.glob(os.path.join(DOCX_FOLDER, \"*.docx\"))\n",
    "print(f\"Found {len(all_paths)} .docx file(s):\")\n",
    "for p in all_paths:\n",
    "    print(\"  ‚Ä¢\", p)\n",
    "\n",
    "# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "# STEP 2: Load each DOCX and wrap as Document\n",
    "# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "reader = DocxReader()\n",
    "raw_documents = []\n",
    "for file_path in all_paths:\n",
    "    docx_pages = reader.load_data(file_path)\n",
    "    for page_obj in docx_pages:\n",
    "        raw_documents.append(\n",
    "            Document(\n",
    "                text=page_obj.text,\n",
    "                metadata={\"source\": os.path.basename(file_path)}\n",
    "            )\n",
    "        )\n",
    "\n",
    "print(f\"Loaded {len(raw_documents)} raw Document(s) from all .docx files.\")\n",
    "\n",
    "# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "# STEP 3: Chunk each Document semantically with metadata\n",
    "# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "nodes = []\n",
    "for doc in raw_documents:\n",
    "    file_name = doc.metadata.get(\"source\", \"\")\n",
    "    attachment_link = f\"{SHAREPOINT_BASE_URL}/{file_name}\"\n",
    "    section_chunks = section_aware_split(doc.text)\n",
    "    \n",
    "    for i, chunk in enumerate(section_chunks):\n",
    "        nodes.append(\n",
    "            Document(\n",
    "                text=chunk[\"content\"],\n",
    "                metadata={\n",
    "                    **doc.metadata,\n",
    "                    \"chunk_id\": i,\n",
    "                    \"section_path\": chunk[\"section_path\"],\n",
    "                    \"level\": chunk[\"level\"],\n",
    "                    \"attachment_link\": f\"{SHAREPOINT_BASE_URL}/{file_name}\"\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "\n",
    "print(f\"After splitting, we have {len(nodes)} chunked Documents (nodes).\")\n",
    "\n",
    "# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "# FINAL: Assign to `documents` so rest of pipeline stays unchanged\n",
    "# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "documents = nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Cohear Embedding service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîë Using Cohere key:    YsB5acLd9Szu9duW5BzCWPfDY1MiDl93MHD2ArYV\n",
      "üî¢ Using Cohere model:  embed-multilingual-light-v3.0\n"
     ]
    }
   ],
   "source": [
    "# ‚Ä¶ (no need to call load_dotenv() here) ‚Ä¶\n",
    "\n",
    "# Hard-code your key and model ID:\n",
    "COHEAR_KEY      = \"YsB5acLd9Szu9duW5BzCWPfDY1MiDl93MHD2ArYV\"\n",
    "COHEAR_MODEL_ID = \"embed-multilingual-light-v3.0\"\n",
    "\n",
    "print(\"üîë Using Cohere key:   \", COHEAR_KEY)\n",
    "print(\"üî¢ Using Cohere model: \", COHEAR_MODEL_ID)\n",
    "\n",
    "embed_model = CohereEmbedding(\n",
    "    api_key=COHEAR_KEY,\n",
    "    model_name=COHEAR_MODEL_ID,\n",
    "    input_type=\"search_document\",\n",
    "    embedding_type=\"float\",\n",
    ")\n",
    "\n",
    "Settings.chunk_size = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Innitiates VectorStore database (Qdrant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "import os\n",
    "\n",
    "QDRANT_URL = \"https://03e32975-0443-4399-ad1c-0c68af34bc38.us-west-2-0.aws.cloud.qdrant.io\"\n",
    "QDRANT_API_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.vESeTOgLX-ILEZQUEOjuJ-6qRQd4i3n0jcwr7dSJAz0\"  # Set this to your Qdrant key if needed\n",
    "COLLECTION_NAME = \"my_collection\"\n",
    "\n",
    "# Initialize Qdrant client with HTTP (not gRPC)\n",
    "client = QdrantClient(\n",
    "    url=QDRANT_URL,  # Using HTTP endpoint exposed by Docker\n",
    "    api_key=QDRANT_API_KEY,\n",
    "    prefer_grpc=False,            # Disable gRPC to avoid connection issues\n",
    "    timeout=60,\n",
    "    check_compatibility=False     # Suppress version mismatch warning\n",
    ")\n",
    "\n",
    "# Load collection name from environment\n",
    "collection_name = os.getenv(\"QDRANT_COLLECTION_NAME\")\n",
    "\n",
    "# Delete collection if it exists\n",
    "if client.collection_exists(collection_name):\n",
    "    client.delete_collection(collection_name)\n",
    "\n",
    "# Create Qdrant vector store with hybrid search enabled\n",
    "vector_store = QdrantVectorStore(\n",
    "    collection_name=collection_name,\n",
    "    client=client,\n",
    "    enable_hybrid=True,\n",
    "    batch_size=20,\n",
    "    prefer_grpc=False             # Match client setting\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents=documents,\n",
    "    embed_model=embed_model,\n",
    "    storage_context=storage_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start embedding process.... into vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ COHERE_API_KEY loaded.\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Hardcoded API key and model config (no .env loading)\n",
    "COHERE_API_KEY = \"YsB5acLd9Szu9duW5BzCWPfDY1MiDl93MHD2ArYV\"\n",
    "COHERE_MODEL_ID = \"embed-multilingual-light-v3.0\"  # <-- replace with your actual model if different\n",
    "\n",
    "QDRANT_URL = \"https://03e32975-0443-4399-ad1c-0c68af34bc38.us-west-2-0.aws.cloud.qdrant.io\"\n",
    "QDRANT_API_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.vESeTOgLX-ILEZQUEOjuJ-6qRQd4i3n0jcwr7dSJAz0\"  # Set this to your Qdrant key if needed\n",
    "COLLECTION_NAME = \"my_collection\"\n",
    "\n",
    "print(\"‚úÖ COHERE_API_KEY loaded.\")\n",
    "\n",
    "# ‚úÖ Initialize Cohere embed model\n",
    "embed_model = CohereEmbedding(\n",
    "    cohere_api_key=COHERE_API_KEY,\n",
    "    model_name=COHERE_MODEL_ID,\n",
    "    input_type=\"search_document\",\n",
    "    embedding_type=\"float\",\n",
    ")\n",
    "\n",
    "# ‚úÖ Build index from documents\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents=documents,\n",
    "    embed_model=embed_model,\n",
    "    storage_context=storage_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try to retrive relavent nodes with question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = CohereEmbedding(\n",
    "    api_key=os.getenv(\"COHERE_API_KEY\"),\n",
    "    model_name=os.getenv(\"COHERE_MODEL_ID\"),\n",
    "    input_type=\"search_query\",\n",
    "    embedding_type=\"float\",\n",
    ")\n",
    "\n",
    "search_query_retriever = index.as_retriever()\n",
    "\n",
    "search_query_retrieved_nodes = search_query_retriever.retrieve(\n",
    "\"Do all Walmart locations offer scan & go?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Node ID:** d36248e0-bacf-4e6d-aba1-985dabeeb5c5<br>**Similarity:** 0.17754348<br>**Text:** # ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ú‡∏•‡∏¥‡∏ï‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏à‡∏≤‡∏Å QA \n",
       "\n",
       "- ‡∏ó‡∏≤‡∏á‡∏ó‡∏µ‡∏° QA ‡∏à‡∏∞‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÅ‡∏•‡∏∞ ‡∏≠‡∏≠‡∏Å‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ú‡∏•‡∏¥‡∏ï‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤ (FR-QA-008/014) ‡πÉ‡∏´‡πâ‡∏Å‡∏±‡∏ö‡∏ù‡πà‡∏≤‡∏¢‡∏à‡∏±‡∏î‡∏ã‡∏∑‡πâ‡∏≠<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core.response.notebook_utils import display_source_node\n",
    "for n in search_query_retrieved_nodes:\n",
    "    display_source_node(n, source_length=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
