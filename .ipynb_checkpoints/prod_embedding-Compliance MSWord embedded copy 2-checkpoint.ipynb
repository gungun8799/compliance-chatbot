{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def section_aware_split(text: str, max_chunk_len: int = 1500) -> list:\n",
    "    \"\"\"\n",
    "    Chunk a Markdown-style document into hierarchical sections (using #, ##, ###) \n",
    "    and return structured chunks with section path and level.\n",
    "    \"\"\"\n",
    "    import re\n",
    "\n",
    "    lines = text.splitlines()\n",
    "    chunks = []\n",
    "    current_chunk_lines = []\n",
    "    current_path = []\n",
    "\n",
    "    def flush_chunk():\n",
    "        if not current_chunk_lines:\n",
    "            return\n",
    "        content = \"\\n\".join(current_chunk_lines).strip()\n",
    "        if content:\n",
    "            chunks.append({\n",
    "                \"section_path\": current_path.copy(),\n",
    "                \"level\": len(current_path),\n",
    "                \"content\": content\n",
    "            })\n",
    "\n",
    "    for line in lines:\n",
    "        header_match = re.match(r\"^(#{1,6})\\s+(.*)\", line)\n",
    "        if header_match:\n",
    "            # New header found\n",
    "            flush_chunk()\n",
    "            level = len(header_match.group(1))\n",
    "            title = header_match.group(2).strip()\n",
    "            current_path = current_path[:level - 1] + [title]\n",
    "            current_chunk_lines = [line]\n",
    "        else:\n",
    "            current_chunk_lines.append(line)\n",
    "\n",
    "    flush_chunk()\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nest_asyncio\n",
    "\n",
    "from llama_index.core import Document, VectorStoreIndex, StorageContext\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core.vector_stores import VectorStoreQueryResult\n",
    "from qdrant_client import QdrantClient, AsyncQdrantClient\n",
    "from llama_index.embeddings.cohere import CohereEmbedding\n",
    "from llama_index.core import Settings\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import os\n",
    "nest_asyncio.apply()\n",
    "load_dotenv(dotenv_path=\".env.dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "\n",
    "def extract_tables_as_markdown(docx_path):\n",
    "    doc = Document(docx_path)\n",
    "    markdown_tables = []\n",
    "    for table in doc.tables:\n",
    "        rows = []\n",
    "        for row in table.rows:\n",
    "            cells = [cell.text.strip() for cell in row.cells]\n",
    "            rows.append(\"| \" + \" | \".join(cells) + \" |\")\n",
    "        if rows:\n",
    "            header = rows[0]\n",
    "            separator = \"| \" + \" | \".join([\"---\"] * len(table.columns)) + \" |\"\n",
    "            markdown_table = \"\\n\".join([header, separator] + rows[1:])\n",
    "            markdown_tables.append(markdown_table)\n",
    "    return markdown_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.file import DocxReader\n",
    "from llama_index.core.schema import Document\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "\n",
    "# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "# CONFIGURATION\n",
    "# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "DOCX_FOLDER = \"documents/\"\n",
    "SHAREPOINT_BASE_URL = \"https://cpaxtra.sharepoint.com/sites/forms-library\"\n",
    "\n",
    "# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "# SECTION TITLE HELPER\n",
    "# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "def extract_section_title(chunk: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract section title from chunk marked as [SECTION] ... or fallback to first line.\n",
    "    \"\"\"\n",
    "    match = re.search(r\"\\[SECTION\\] (.*?)\\n\", chunk)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    # fallback to first non-empty line\n",
    "    lines = [line.strip() for line in chunk.splitlines() if line.strip()]\n",
    "    return lines[0] if lines else \"unknown\"\n",
    "\n",
    "# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "# STEP 1: Discover all .docx files\n",
    "# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "all_paths = glob.glob(os.path.join(DOCX_FOLDER, \"*.docx\"))\n",
    "print(f\"Found {len(all_paths)} .docx file(s):\")\n",
    "for p in all_paths:\n",
    "    print(\"  â€¢\", p)\n",
    "\n",
    "# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "# STEP 2: Load each DOCX and wrap as Document\n",
    "# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "reader = DocxReader()\n",
    "raw_documents = []\n",
    "for file_path in all_paths:\n",
    "    docx_pages = reader.load_data(file_path)\n",
    "    for page_obj in docx_pages:\n",
    "        raw_documents.append(\n",
    "            Document(\n",
    "                text=page_obj.text,\n",
    "                metadata={\"source\": os.path.basename(file_path)}\n",
    "            )\n",
    "        )\n",
    "\n",
    "print(f\"Loaded {len(raw_documents)} raw Document(s) from all .docx files.\")\n",
    "\n",
    "# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "# STEP 3: Chunk each Document semantically with metadata\n",
    "# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "nodes = []\n",
    "for doc in raw_documents:\n",
    "    file_name = doc.metadata.get(\"source\", \"\")\n",
    "    attachment_link = f\"{SHAREPOINT_BASE_URL}/{file_name}\"\n",
    "    section_chunks = section_aware_split(doc.text)\n",
    "    \n",
    "    for i, chunk in enumerate(section_chunks):\n",
    "        nodes.append(\n",
    "            Document(\n",
    "                text=chunk[\"content\"],\n",
    "                metadata={\n",
    "                    **doc.metadata,\n",
    "                    \"chunk_id\": i,\n",
    "                    \"section_path\": chunk[\"section_path\"],\n",
    "                    \"level\": chunk[\"level\"],\n",
    "                    \"attachment_link\": f\"{SHAREPOINT_BASE_URL}/{file_name}\"\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "\n",
    "print(f\"After splitting, we have {len(nodes)} chunked Documents (nodes).\")\n",
    "\n",
    "# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "# FINAL: Assign to `documents` so rest of pipeline stays unchanged\n",
    "# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "documents = nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Cohear Embedding service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â€¦ (no need to call load_dotenv() here) â€¦\n",
    "\n",
    "# Hard-code your key and model ID:\n",
    "COHEAR_KEY      = \"Iyn2rmOdEgiUKfxptJDhCKRwgfeIWhZ37sxzKUAc\"\n",
    "COHEAR_MODEL_ID = \"embed-multilingual-light-v3.0\"\n",
    "\n",
    "print(\"ðŸ”‘ Using Cohere key:   \", COHEAR_KEY)\n",
    "print(\"ðŸ”¢ Using Cohere model: \", COHEAR_MODEL_ID)\n",
    "\n",
    "embed_model = CohereEmbedding(\n",
    "    api_key=COHEAR_KEY,\n",
    "    model_name=COHEAR_MODEL_ID,\n",
    "    input_type=\"search_document\",\n",
    "    embedding_type=\"float\",\n",
    ")\n",
    "\n",
    "Settings.chunk_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.text_embeddings_inference import TextEmbeddingsInference\n",
    "\n",
    "# Initialize the Qdrant client\n",
    "# Initialize the embedding settings\n",
    "embed_model = TextEmbeddingsInference(\n",
    "    model_name=os.getenv(\"EMBED_MODEL_ID\"),\n",
    "    base_url=os.getenv(\"EMBED_BASE_URL\"),\n",
    "    auth_token=f\"Bearer {os.getenv('API_KEY_CHATBOT')}\",\n",
    "    timeout=60,\n",
    "    embed_batch_size=10,\n",
    ")\n",
    "\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Innitiates VectorStore database (Qdrant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "import os\n",
    "\n",
    "# Initialize Qdrant client with HTTP (not gRPC)\n",
    "client = QdrantClient(\n",
    "    url=\"http://localhost:6433\",  # Using HTTP endpoint exposed by Docker\n",
    "    api_key=os.getenv(\"QDRANT_API_KEY\"),\n",
    "    prefer_grpc=False,            # Disable gRPC to avoid connection issues\n",
    "    timeout=60,\n",
    "    check_compatibility=False     # Suppress version mismatch warning\n",
    ")\n",
    "\n",
    "# Load collection name from environment\n",
    "collection_name = os.getenv(\"QDRANT_COLLECTION_NAME\")\n",
    "\n",
    "# Delete collection if it exists\n",
    "if client.collection_exists(collection_name):\n",
    "    client.delete_collection(collection_name)\n",
    "\n",
    "# Create Qdrant vector store with hybrid search enabled\n",
    "vector_store = QdrantVectorStore(\n",
    "    collection_name=collection_name,\n",
    "    client=client,\n",
    "    enable_hybrid=True,\n",
    "    batch_size=20,\n",
    "    prefer_grpc=False             # Match client setting\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start embedding process.... into vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Hardcoded API key and model config (no .env loading)\n",
    "COHERE_API_KEY = \"Iyn2rmOdEgiUKfxptJDhCKRwgfeIWhZ37sxzKUAc\"\n",
    "COHERE_MODEL_ID = \"embed-multilingual-light-v3.0\"  # <-- replace with your actual model if different\n",
    "\n",
    "QDRANT_URL = \"http://localhost:6334\"\n",
    "QDRANT_API_KEY = None  # Set this to your Qdrant key if needed\n",
    "COLLECTION_NAME = \"my_collection\"\n",
    "\n",
    "print(\"âœ… COHERE_API_KEY loaded.\")\n",
    "\n",
    "# âœ… Initialize Cohere embed model\n",
    "embed_model = CohereEmbedding(\n",
    "    cohere_api_key=COHERE_API_KEY,\n",
    "    model_name=COHERE_MODEL_ID,\n",
    "    input_type=\"search_document\",\n",
    "    embedding_type=\"float\",\n",
    ")\n",
    "\n",
    "# âœ… Build index from documents\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents=documents,\n",
    "    embed_model=embed_model,\n",
    "    storage_context=storage_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try to retrive relavent nodes with question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = CohereEmbedding(\n",
    "    api_key=os.getenv(\"COHERE_API_KEY\"),\n",
    "    model_name=os.getenv(\"COHERE_MODEL_ID\"),\n",
    "    input_type=\"search_query\",\n",
    "    embedding_type=\"float\",\n",
    ")\n",
    "\n",
    "search_query_retriever = index.as_retriever()\n",
    "\n",
    "search_query_retrieved_nodes = search_query_retriever.retrieve(\n",
    "\"Do all Walmart locations offer scan & go?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.response.notebook_utils import display_source_node\n",
    "for n in search_query_retrieved_nodes:\n",
    "    display_source_node(n, source_length=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
