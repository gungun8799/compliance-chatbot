{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/th40124157/projects/compliance-chatbot/.conda/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-06-25 15:52:22,410 - INFO - Patched TextEmbeddingsInference._call_api with custom synchronous API handling\n",
      "2025-06-25 15:52:22,411 - INFO - Patched TextEmbeddingsInference._acall_api with custom asynchronous API handling\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import nest_asyncio\n",
    "from llama_index.core import Document, VectorStoreIndex\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient, AsyncQdrantClient\n",
    "from llama_index.embeddings.text_embeddings_inference import TextEmbeddingsInference\n",
    "from llama_index.embeddings.cohere import CohereEmbedding\n",
    "from llama_index.core import Settings\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "# Apply the monkey patch\n",
    "from chainlit_app.patches import patch\n",
    "\n",
    "patch.apply_patch()\n",
    "nest_asyncio.apply()\n",
    "load_dotenv(dotenv_path=\".env.dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "\n",
    "def extract_tables_as_markdown(docx_path):\n",
    "    doc = Document(docx_path)\n",
    "    markdown_tables = []\n",
    "    for table in doc.tables:\n",
    "        rows = []\n",
    "        for row in table.rows:\n",
    "            cells = [cell.text.strip() for cell in row.cells]\n",
    "            rows.append(\"| \" + \" | \".join(cells) + \" |\")\n",
    "        if rows:\n",
    "            header = rows[0]\n",
    "            separator = \"| \" + \" | \".join([\"---\"] * len(table.columns)) + \" |\"\n",
    "            markdown_table = \"\\n\".join([header, separator] + rows[1:])\n",
    "            markdown_tables.append(markdown_table)\n",
    "    return markdown_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15 .docx file(s):\n",
      "  • documents/FA-G-02_StaffReimbursement_SectionAware.docx\n",
      "  • documents/อำนาจอนุมัติรายจ่ายสำหรับ Purchase Requisition_แปลงตาราง.docx\n",
      "  • documents/CPAX-FN-005_ProjectInvestment_SectionAware.docx\n",
      "  • documents/Trade Supplier Registration and Payment Policy_FA-G-15_15012025_for sign.docx\n",
      "  • documents/FA-G-02 (T) Staff Expense Reimbursement _15012025 (Chat bot).docx\n",
      "  • documents/CPAX-FN-005_Project Investment_TH_Final_Narrative_Chatbot.docx\n",
      "  • documents/FA-G-15_SectionAware_Final_Sectioned.docx\n",
      "  • documents/FA-G-07 Non-Trade Supplier (Chat bot).docx\n",
      "  • documents/FA-G-17  Tenant Selection and Debt Collection_Chatbot).docx\n",
      "  • documents/FA-G-17_SectionAwareChunking.docx\n",
      "  • documents/ระเบียบปฏิบัติ เรื่อง อำนาจอนุมัติ Level of Authorization_แปลงตาราง.docx\n",
      "  • documents/FA-G-07_NonTradeSupplier_SectionAware.docx\n",
      "  • documents/FA-B2B-01_CreditMgmt_SectionAware.docx\n",
      "  • documents/FA-G-08 อำนาจอนุมัติรายจ่ายสำหรับ Purchase Requisition_แปลงตาราง.docx\n",
      "  • documents/FA-B2B-01 Credit Management for B2B_10032025_for sign_chatbot_Final_v1.docx\n",
      "Loaded 15 raw Document(s) from all .docx files.\n",
      "After splitting, we have 257 chunked Documents (nodes).\n",
      "Final documents ready for indexing: 257 nodes.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.readers.file import DocxReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.schema import Document\n",
    "import glob\n",
    "\n",
    "# ——————————————\n",
    "# CONFIGURATION\n",
    "# ——————————————\n",
    "\n",
    "# 1) Directory where all your .docx files live\n",
    "DOCX_FOLDER = \"documents/\"\n",
    "\n",
    "# ——————————————\n",
    "# STEP 1: Discover all .docx files\n",
    "# ——————————————\n",
    "\n",
    "all_paths = glob.glob(os.path.join(DOCX_FOLDER, \"*.docx\"))\n",
    "print(f\"Found {len(all_paths)} .docx file(s):\")\n",
    "for p in all_paths:\n",
    "    print(\"  •\", p)\n",
    "\n",
    "# ——————————————\n",
    "# STEP 2: Load each DOCX and wrap as Document\n",
    "# ——————————————\n",
    "\n",
    "reader = DocxReader()\n",
    "raw_documents = []\n",
    "for file_path in all_paths:\n",
    "    # load_data returns a list of in‐memory “page” objects \n",
    "    docx_pages = reader.load_data(file_path)\n",
    "    for page_obj in docx_pages:\n",
    "        raw_documents.append(\n",
    "            Document(\n",
    "                text=page_obj.text,\n",
    "                metadata={\"source\": os.path.basename(file_path)}\n",
    "            )\n",
    "        )\n",
    "\n",
    "print(f\"Loaded {len(raw_documents)} raw Document(s) from all .docx files.\")\n",
    "\n",
    "# ——————————————\n",
    "# STEP 3: Chunk each Document semantically\n",
    "# ——————————————\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "\n",
    "nodes = []\n",
    "for doc in raw_documents:\n",
    "    chunks = splitter.split_text(doc.text)\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        nodes.append(\n",
    "            Document(\n",
    "                text=chunk,\n",
    "                metadata={**doc.metadata, \"chunk_id\": i}\n",
    "            )\n",
    "        )\n",
    "\n",
    "print(f\"After splitting, we have {len(nodes)} chunked Documents (nodes).\")\n",
    "\n",
    "# ——————————————\n",
    "# FINAL: Assign to `documents` so the rest of your pipeline can stay unchanged\n",
    "# ——————————————\n",
    "\n",
    "documents = nodes\n",
    "print(f\"Final documents ready for indexing: {len(documents)} nodes.\")\n",
    "\n",
    "# Now you can call your index creation exactly as before:\n",
    "# from llama_index.core.embeddings import CohereEmbedding  # or whichever embed_model you use\n",
    "# from llama_index.core.storage import StorageContext\n",
    "# from llama_index.vector_stores import QdrantVectorStore\n",
    "# from llama_index import VectorStoreIndex\n",
    "\n",
    "# Example (adjust embed_model, storage_context, etc. to your configuration):\n",
    "# storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "# index = VectorStoreIndex.from_documents(\n",
    "#     documents=documents,\n",
    "#     embed_model=Settings.embed_model,\n",
    "#     storage_context=storage_context,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Embedding service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Qdrant client\n",
    "# Initialize the embedding settings\n",
    "embed_model = TextEmbeddingsInference(\n",
    "    model_name=os.getenv(\"EMBED_MODEL_ID\"),\n",
    "    base_url=os.getenv(\"EMBED_BASE_URL\"),\n",
    "    auth_token=f\"Bearer {os.getenv(\"API_KEY_CHATBOT\")}\",\n",
    "    timeout=60,\n",
    "    embed_batch_size=10,\n",
    ")\n",
    "\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Innitiates VectorStore database (Qdrant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to connect to Qdrant gRPC on host: localhost, port: 6434\n",
      "Using Qdrant Collection Name: FAQ_DATA\n",
      "Collection FAQ_DATA does not exist, skipping deletion.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nq/1pf_sjxx2hqd1s78blj1_6xr0000gn/T/ipykernel_76294/149784803.py:17: UserWarning: Api key is used with an insecure connection.\n",
      "  client = QdrantClient(\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables\n",
    "qdrant_api_key = os.getenv(\"QDRANT_API_KEY\")\n",
    "collection_name = os.getenv(\"QDRANT_COLLECTION_NAME\")\n",
    "\n",
    "# Your docker-compose maps the gRPC port to 6434 on the host.\n",
    "qdrant_grpc_port = 6434\n",
    "qdrant_host = \"localhost\"\n",
    "\n",
    "print(f\"Attempting to connect to Qdrant gRPC on host: {qdrant_host}, port: {qdrant_grpc_port}\")\n",
    "print(f\"Using Qdrant Collection Name: {collection_name}\")\n",
    "\n",
    "if not collection_name:\n",
    "    raise ValueError(\"QDRANT_COLLECTION_NAME environment variable is not set\")\n",
    "\n",
    "# Correctly initialize the client to use the mapped gRPC port\n",
    "# AND explicitly disable HTTPS/TLS to match the server configuration.\n",
    "client = QdrantClient(\n",
    "    host=qdrant_host,\n",
    "    grpc_port=qdrant_grpc_port,\n",
    "    api_key=qdrant_api_key,\n",
    "    prefer_grpc=True,\n",
    "    https=False # <--- THIS IS THE FIX\n",
    ")\n",
    "\n",
    "\n",
    "# delete collection if it exists\n",
    "if client.collection_exists(collection_name):\n",
    "    print(f\"Deleting existing collection: {collection_name}\")\n",
    "    client.delete_collection(collection_name)\n",
    "else:\n",
    "    print(f\"Collection {collection_name} does not exist, skipping deletion.\")\n",
    "    \n",
    "\n",
    "# create our vector store with hybrid indexing enabled\n",
    "vector_store = QdrantVectorStore(\n",
    "    collection_name=os.getenv(\"QDRANT_COLLECTION_NAME\"),\n",
    "    client=client,\n",
    "    enable_hybrid=True,\n",
    "    batch_size=20,\n",
    "    prefer_grpc=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start embedding process.... into vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-25 15:52:29,098 - INFO - HTTP Request: POST https://api.cpxis.global.lotuss.org/embedding/BAAI/bge-m3/embed \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 15:52:29,703 - INFO - HTTP Request: POST https://api.cpxis.global.lotuss.org/embedding/BAAI/bge-m3/embed \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 15:52:30,249 - INFO - HTTP Request: POST https://api.cpxis.global.lotuss.org/embedding/BAAI/bge-m3/embed \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 15:52:30,707 - INFO - HTTP Request: POST https://api.cpxis.global.lotuss.org/embedding/BAAI/bge-m3/embed \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 15:52:31,148 - INFO - HTTP Request: POST https://api.cpxis.global.lotuss.org/embedding/BAAI/bge-m3/embed \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 15:52:31,612 - INFO - HTTP Request: POST https://api.cpxis.global.lotuss.org/embedding/BAAI/bge-m3/embed \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 15:52:32,086 - INFO - HTTP Request: POST https://api.cpxis.global.lotuss.org/embedding/BAAI/bge-m3/embed \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 15:52:32,532 - INFO - HTTP Request: POST https://api.cpxis.global.lotuss.org/embedding/BAAI/bge-m3/embed \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 15:52:32,994 - INFO - HTTP Request: POST https://api.cpxis.global.lotuss.org/embedding/BAAI/bge-m3/embed \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 15:52:33,475 - INFO - HTTP Request: POST https://api.cpxis.global.lotuss.org/embedding/BAAI/bge-m3/embed \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 15:52:33,940 - INFO - HTTP Request: POST https://api.cpxis.global.lotuss.org/embedding/BAAI/bge-m3/embed \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 15:52:34,469 - INFO - HTTP Request: POST https://api.cpxis.global.lotuss.org/embedding/BAAI/bge-m3/embed \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 15:52:35,012 - INFO - HTTP Request: POST https://api.cpxis.global.lotuss.org/embedding/BAAI/bge-m3/embed \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 15:52:35,621 - INFO - HTTP Request: POST https://api.cpxis.global.lotuss.org/embedding/BAAI/bge-m3/embed \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 15:52:36,159 - INFO - HTTP Request: POST https://api.cpxis.global.lotuss.org/embedding/BAAI/bge-m3/embed \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 15:52:36,731 - INFO - HTTP Request: POST https://api.cpxis.global.lotuss.org/embedding/BAAI/bge-m3/embed \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 15:52:37,231 - INFO - HTTP Request: POST https://api.cpxis.global.lotuss.org/embedding/BAAI/bge-m3/embed \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 15:52:37,821 - INFO - HTTP Request: POST https://api.cpxis.global.lotuss.org/embedding/BAAI/bge-m3/embed \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 15:52:38,368 - INFO - HTTP Request: POST https://api.cpxis.global.lotuss.org/embedding/BAAI/bge-m3/embed \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 15:52:38,875 - INFO - HTTP Request: POST https://api.cpxis.global.lotuss.org/embedding/BAAI/bge-m3/embed \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 15:52:39,402 - INFO - HTTP Request: POST https://api.cpxis.global.lotuss.org/embedding/BAAI/bge-m3/embed \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 15:52:39,929 - INFO - HTTP Request: POST https://api.cpxis.global.lotuss.org/embedding/BAAI/bge-m3/embed \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 15:52:40,506 - INFO - HTTP Request: POST https://api.cpxis.global.lotuss.org/embedding/BAAI/bge-m3/embed \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 15:52:40,967 - INFO - HTTP Request: POST https://api.cpxis.global.lotuss.org/embedding/BAAI/bge-m3/embed \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 15:52:41,432 - INFO - HTTP Request: POST https://api.cpxis.global.lotuss.org/embedding/BAAI/bge-m3/embed \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 15:52:41,962 - INFO - HTTP Request: POST https://api.cpxis.global.lotuss.org/embedding/BAAI/bge-m3/embed \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 15:52:42,486 - INFO - HTTP Request: POST https://api.cpxis.global.lotuss.org/embedding/BAAI/bge-m3/embed \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents=documents, embed_model=embed_model, storage_context=storage_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try to retrive relavent nodes with question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-25 15:53:27,169 - INFO - HTTP Request: POST https://api.cpxis.global.lotuss.org/embedding/BAAI/bge-m3/embed \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "search_query_retriever = index.as_retriever()\n",
    "\n",
    "search_query_retrieved_nodes = search_query_retriever.retrieve(\n",
    "\"Do all Walmart locations offer scan & go?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 099f72d9-22f2-4ad7-8cb4-36592eda6766<br>**Similarity:** 0.4412797689437866<br>**Text:** สายบังคับบัญชาของทีมขาย B2B\n",
       "\n",
       "พนักงานขายของ Lotus’s : \n",
       "\n",
       "Go-fresh : ผู้จัดการทั่วไป (Area General Manager – AGM)\n",
       "\n",
       "Hypermarket พนักงานขายในสาขา : ผู้จัดการสาขา (Store Manager) ->ผู้จัดการทั่วไป (Area General Manager – AGM)\n",
       "\n",
       "Hypermarket พนักงานขายนอกสาขา : ผู้จัดการเขตขาย (Zone Manager) -> ผู้จัดการอาวุโสเขตขาย (Senior Zone Manager)\n",
       "\n",
       "พนักงานขายของ Makro : ผู้จัดการฝ่ายขาย (Sales Manager) ->ผู้จัดการฝ่ายขายประจำภูมิภาค (Regional Sales Manager)\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "คำถามอื่นๆของ B2B ที่นอกเหนือ Policy\n",
       "\n",
       "หัวข้อคำถามเกี่ยวกับเรื่อง  Open new CV - เปิดหน้าบัญชีลูกค้าใหม่  คำถาม : ลูกค้ารายนี้เคยเปิดหน้าบัญชี หรือเคยมีการซื้อขายกับบริษัทฯมาก่อนหรือไม่?  คำตอบ : ขอให้ตรวจสอบข้อมูลของลูกค้าโดยใช้เลขประจำตัวผู้เสียภาษี 13 หลัก (Tax ID) เข้าไปตรวจสอบในระบบ smartsoft\n",
       "\n",
       "หัวข้อคำถามเกี่ยวกับเรื่อง  Open new CV - เปิดหน้าบัญชีลูกค้าใหม่  คำถาม : ต้องใช้เอกสารอะไรบ้างในการเปิดบัญชีลูกค้าใหม่?  คำตอบ : กรณีลูกค้าเป็นบุคคลธรรมดาต้องแนบเอกสารสำคัญดังนี้ สำเนาใบเปิดบัญชีลูกค้า+สำเนาบัตรประจำตัวประชาชนหรือบัตรข้าราชการของลูกค้า/เจ้าของ/ผู้ประกอบการ/หุ้นส่วนผู้จัดการ/กรรมการผู้มีอำนาจ+รูปถ่ายเซลฟี่ของพนักงานขายกับสถานประกอบการ (ต้องเห็นป้ายหน้าร้าน/บริษัทฯ) และหากมีเอกสารเหล่านี้ให้แนบมาด้วย คือสำเนาหนังสือรับรองการจดทะเบียนพาณิชย์หรือสำเนาหังสือจัดตั้งหุ้นส่วนสามัญและ/หรือสำเนาใบทะเบียนภาษีมูลค่าเพิ่ม(ภพ.<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 94c66b38-f58b-4ced-a131-511254da6181<br>**Similarity:** 0.4396381378173828<br>**Text:** ร้านโชห่วย, ร้านมินิมาร์ท, Mom & Pop เครดิตเทอมมาตรฐาน บุคคลธรรมดา ไม่ให้เครดิตเทอม (0 วัน) นิติบุคคล ไม่ให้เครดิตเทอม (0 วัน)\n",
       "\n",
       "ช่องทางขาย สถาบันการศึกษา หน่วยงานราชการ รัฐวิสาหกิจ โรงพยาบาล และสถาบันต่างๆ เครดิตเทอมมาตรฐาน นิติบุคคล 30 วัน\n",
       "\n",
       "ช่องทางขาย ปั๊มน้ำมัน – จัดซื้อส่วนกลาง เครดิตเทอมมาตรฐาน บุคคลธรรมดา ไม่เกิน 5 วัน นิติบุคคล ไม่เกิน 45 วัน\n",
       "\n",
       "ช่องทางขาย ปั๊มน้ำมัน – จัดซื้อแยกสาขา เครดิตเทอมมาตรฐาน บุคคลธรรมดา ไม่เกิน 5 วัน นิติบุคคล ไม่เกิน 15 วัน\n",
       "\n",
       "ช่องทางขาย ร้านขายยา เครดิตเทอมมาตรฐาน บุคคลธรรมดา 5 วัน นิติบุคคล 15 วัน\n",
       "\n",
       "ช่องทางขาย ร้านค้าส่ง (Wholesales),<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core.response.notebook_utils import display_source_node\n",
    "for n in search_query_retrieved_nodes:\n",
    "    display_source_node(n, source_length=2000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
